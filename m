Return-Path: <linux-kernel-owner+willy=40w.ods.org@vger.kernel.org>
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id <S315454AbSGQRd1>; Wed, 17 Jul 2002 13:33:27 -0400
Received: (majordomo@vger.kernel.org) by vger.kernel.org
	id <S315921AbSGQRd1>; Wed, 17 Jul 2002 13:33:27 -0400
Received: from fmr04.intel.com ([143.183.121.6]:20955 "EHLO
	caduceus.sc.intel.com") by vger.kernel.org with ESMTP
	id <S315454AbSGQRdY>; Wed, 17 Jul 2002 13:33:24 -0400
Message-Id: <200207171735.g6HHZUP28835@unix-os.sc.intel.com>
Content-Type: text/plain;
  charset="iso-8859-1"
From: mgross <mgross@unix-os.sc.intel.com>
Reply-To: mgross@unix-os.sc.intel.com
Organization: SSG Intel
To: Andrea Arcangeli <andrea@suse.de>,
       "Griffiths, Richard A" <richard.a.griffiths@intel.com>
Subject: Re: fsync fixes for 2.4
Date: Wed, 17 Jul 2002 10:44:18 -0400
X-Mailer: KMail [version 1.3.1]
Cc: "'Andrew Morton'" <akpm@zip.com.au>,
       "'Marcelo Tosatti'" <marcelo@conectiva.com.br>,
       "'linux-kernel@vger.kernel.org'" <linux-kernel@vger.kernel.org>,
       "'Carter K. George'" <carter@polyserve.com>,
       "'Don Norton'" <djn@polyserve.com>,
       "'James S. Tybur'" <jtybur@polyserve.com>,
       "Gross, Mark" <mark.gross@intel.com>
References: <01BDB7EEF8D4D3119D95009027AE99951B0E6428@fmsmsx33.fm.intel.com> <20020715100719.GE34@dualathlon.random>
In-Reply-To: <20020715100719.GE34@dualathlon.random>
MIME-Version: 1.0
Content-Transfer-Encoding: 8bit
Sender: linux-kernel-owner@vger.kernel.org
X-Mailing-List: linux-kernel@vger.kernel.org

On Monday 15 July 2002 06:07 am, Andrea Arcangeli wrote:
> On Fri, Jul 12, 2002 at 02:52:11PM -0700, Griffiths, Richard A wrote:
> > Mark is off climbing Mt. Hood, so he asked me to post the data on the
> > fsync patch.

I was excited to report the significant improvement of 2.4.19rc1+fsync fix 
over 2.4.18 and didn't realize that the improvement was not due to the fsync 
patch.   I'm so glad Richard did a careful check, I was on my way out the 
door for my vacation :)

I would like to know what's so good about 2.4.19rc1 that gives our block I/O 
benchmark that significant improvement over 2.4.18.


> > It appears from these results that there is no appreciable improvement
> > using the
> > fsync patch - there is a slight loss of top end on 4 adapters 1 drive.
>
> that's very much expected, as said with my new design by adding an
> additional pass (third pass), I could remove the slight loss that I
> expected from the simple patch that puts wait_on_buffer right in the
> first pass.
>
> I mentioned this in my first email of the thread, so it looks all right.
> For a rc2 the slight loss sounds like the simplest approch.
>
> If you care about it, with my new fsync accounting design we can fix it,
> just let me know if you're interested about it. Personally I'm pretty
> much fine with it this way too, as said in the first email if we block
> it's likely bdflush is pumping the queue for us. the slowdown is most
> probably due too early unplug of the queue generated by the blocking
> points.

I don't care about the very slight (and possibly in the noise floor of our 
test) reduction in throughput due to the fsync fix.  I think your's and 
Andrews'  assertion of the bdflush / dirty page handling getting stopped up 
is likely the problem preventing scaling to my personal goal of 250 to 
300MB/sec on our setup.

Thanks,

Mark Gross
PS I had a very nice time on mount hood.  I didn't make it to the top this 
time too much snow had melted off the top of the thing to have a safe attempt 
at the summit.  It was a guided (http://www.timberlinemtguides.com) 3 day 
climb. 
