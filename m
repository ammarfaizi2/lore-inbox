Return-Path: <linux-kernel-owner+willy=40w.ods.org-S1751628AbWAOCF0@vger.kernel.org>
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1751628AbWAOCF0 (ORCPT <rfc822;willy@w.ods.org>);
	Sat, 14 Jan 2006 21:05:26 -0500
Received: (majordomo@vger.kernel.org) by vger.kernel.org id S1751635AbWAOCFZ
	(ORCPT <rfc822;linux-kernel-outgoing>);
	Sat, 14 Jan 2006 21:05:25 -0500
Received: from mail13.syd.optusnet.com.au ([211.29.132.194]:1752 "EHLO
	mail13.syd.optusnet.com.au") by vger.kernel.org with ESMTP
	id S1751527AbWAOCFZ (ORCPT <rfc822;linux-kernel@vger.kernel.org>);
	Sat, 14 Jan 2006 21:05:25 -0500
From: Con Kolivas <kernel@kolivas.org>
To: Peter Williams <pwil3058@bigpond.net.au>
Subject: Re: -mm seems significanty slower than mainline on kernbench
Date: Sun, 15 Jan 2006 13:04:50 +1100
User-Agent: KMail/1.9
Cc: "Martin J. Bligh" <mbligh@google.com>, Andrew Morton <akpm@osdl.org>,
       linux-kernel@vger.kernel.org, Ingo Molnar <mingo@elte.hu>,
       Andy Whitcroft <apw@shadowen.org>,
       Nick Piggin <nickpiggin@yahoo.com.au>
References: <43C45BDC.1050402@google.com> <43C9477B.8060709@google.com> <43C991D0.3040808@bigpond.net.au>
In-Reply-To: <43C991D0.3040808@bigpond.net.au>
MIME-Version: 1.0
Content-Type: text/plain;
  charset="iso-8859-1"
Content-Transfer-Encoding: 7bit
Content-Disposition: inline
Message-Id: <200601151304.52345.kernel@kolivas.org>
Sender: linux-kernel-owner@vger.kernel.org
X-Mailing-List: linux-kernel@vger.kernel.org

On Sunday 15 January 2006 11:05, Peter Williams wrote:
> Martin J. Bligh wrote:
> >> Attached is a new patch to fix the excessive idle problem.  This patch
> >> takes a new approach to the problem as it was becoming obvious that
> >> trying to alter the load balancing code to cope with biased load was
> >> harder than it seemed.
> >>
> >> This approach reverts to the old load values but weights them
> >> according to tasks' bias_prio values.  This means that any assumptions
> >> by the load balancing code that the load generated by a single task is
> >> SCHED_LOAD_SCALE will still hold.  Then, in find_busiest_group(), the
> >> imbalance is scaled back up to bias_prio scale so that move_tasks()
> >> can move biased load rather than tasks.
> >
> > OK, this one seems to fix the issue that I had, AFAICS. Congrats, and
> > thanks,
>
> Terrific, thanks for testing.
>
> Con,
> 	Attached is a cleaned up version of this patch against 2.6.15-mm4 with
> some (hopefully helpful) comments added.

Great! Well done.

> Signed-off-by: Peter Williams <pwil3058@bigpond.com.au>

Acked-by: Con Kolivas <kernel@kolivas.org>
