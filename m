Return-Path: <linux-kernel-owner+willy=40w.ods.org-S1750884AbWCBOih@vger.kernel.org>
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1750884AbWCBOih (ORCPT <rfc822;willy@w.ods.org>);
	Thu, 2 Mar 2006 09:38:37 -0500
Received: (majordomo@vger.kernel.org) by vger.kernel.org id S1750947AbWCBOih
	(ORCPT <rfc822;linux-kernel-outgoing>);
	Thu, 2 Mar 2006 09:38:37 -0500
Received: from omx2-ext.sgi.com ([192.48.171.19]:59554 "EHLO omx2.sgi.com")
	by vger.kernel.org with ESMTP id S1750922AbWCBOih (ORCPT
	<rfc822;linux-kernel@vger.kernel.org>);
	Thu, 2 Mar 2006 09:38:37 -0500
Date: Thu, 2 Mar 2006 06:38:22 -0800 (PST)
From: Christoph Lameter <clameter@engr.sgi.com>
To: Andi Kleen <ak@suse.de>
cc: Paul Jackson <pj@sgi.com>, dgc@sgi.com, steiner@sgi.com,
       Simon.Derr@bull.net, linux-kernel@vger.kernel.org
Subject: Re: [PATCH 01/02] cpuset memory spread slab cache filesys
In-Reply-To: <200603020257.29234.ak@suse.de>
Message-ID: <Pine.LNX.4.64.0603020629070.4111@schroedinger.engr.sgi.com>
References: <20060227070209.1994.26823.sendpatchset@jackhammer.engr.sgi.com>
 <200603012221.37271.ak@suse.de> <Pine.LNX.4.64.0603011411190.31997@schroedinger.engr.sgi.com>
 <200603020257.29234.ak@suse.de>
MIME-Version: 1.0
Content-Type: TEXT/PLAIN; charset=US-ASCII
Sender: linux-kernel-owner@vger.kernel.org
X-Mailing-List: linux-kernel@vger.kernel.org

On Thu, 2 Mar 2006, Andi Kleen wrote:

> (often in my measurements 40% and more of all syscalls are gettimeofday 
> actually)

Yes that is why we excessively optimized gettimeofday in asm on ia64...

> Also we don't have very good balancing control on dcaches.

Right but I hope we will get there if we can get the zoned vm statistics 
in and rework the VM a bit.

> > Please run performance tests with single threaded processes if you 
> > do not believe me before doing any of this.
> 
> Sure. But the motivation is less the single thread performance
> anyways, but more the degradation under extreme loads.

The extreme loads may benefit from interleave. But note that the 
performance gains in the NUMA slab allocator came from exploiting 
locality. The support of policies and other off node memory accesses in 
the SLAB allocator is an afterthought. Only node local accesses can be 
served from per cpu caches with a simple interrupt on / off. Off node 
accesses generated by policies etc will require locking and working with 
remote memory structures.

If these are indeed rare user space accesses that require slab elements 
then these performance issues do not matter and you may be able to spread 
without performance penalties. However, our experience with the slab 
allocator was that intensive workloads can be influenced by slab 
performance.

